<!DOCTYPE html>
<html>
    <head>


        <script src="lib/jquery.min.js"></script>
        <script type="text/javascript" src="visualization/d3.v5.js" ></script>

         <link type="text/css" rel="stylesheet" href="lib/materialize/css/materialize.min.css"  media="screen,projection"/>
         <link rel="stylesheet" type="text/css" href="mystyle.css">
    </head>
<body>


  <nav class="nav-center" role="navigation">
     <div class="nav-wrapper">

         <ul id="nav-mobile">
            <li><a href="index.html">Home</a></li>
            <!-- <li><a href="examples.html">Gallery</a></li> -->
         </ul>
     </div>
 </nav>
 <!-- <div class="intro-header">

</div> -->



    <div id='image'>

        
    </div>

    <div id= "content">
        <div class="row">
            <div class="col m8 offset-m2 headTitle center">
            <h4>Air Salut: Sensing and Interacting across Ad-Hoc Device
                Ecologies by Continuous In-Air Hand Gesture</h4>
            </div>
        
            <div class="col m6 offset-m3 justify">
                Digital workspaces are increasingly spanning across multiple connected computing devices. Yet, the user experience across device ecologies is often fragmented, optimized for individual device use, and lacking appropriate cross-connecting interactions and workflows. We introduce PersonalFleet, an ad-hoc cross-device environment leveraging spatial sensing and in-air gestural interactions to blur the boundaries between individual devices, across form factors and input/output modalities. We lay out the design space of continuous cross-modal gestures around personal device ecologiesâ€”introducing a design vocabulary of gesture/target granularity, scale, distance, among other dimensions. Example applications illustrate mappings of these dimensions to single or bi-manual hand gestures coupling in-air and touch inputs mediating workflows across devices: e.g., redirecting input, transferring content, or migrating whole application states. We introduce visual feedback and feed-forward techniques facilitating the use of this new gesture vocabulary, and explain details of our LiDAR-based proof-of-concept implementation, such as {\color{blue} reconstruction of devices physical relations using cross-devices} gesture trajectories. Findings from our user study highlights trade-offs with coarse in-air gestures, challenges of natural vs. designed gestures, and potential for flexible gesture coupled with the entire range of modalities available.
            </div>

            <br/>
            <br/>
            <br/>
            <br/>
            <br/>
            <br/>
        </div>
    
        
        <!-- <div class="col m11 offset-m1 headTitle" style="font-size:180%">
            Screenshots fron usecases
        <hr>
        </div>
        <div class="row" class='examples'>
            <div class="col s4 offset-s2"><img src='images/capture (1).PNG' /></div>
            <div class="col s4"><img src='images/capture (2).png' /></div>
            
        </div>    
        <div class="row" class='examples'>
            <div class="col s4 offset-s2"><img src='images/capture (3).png' /></div>
            <div class="col s4"><img src='images/capture (4).png' /></div>
        </div>
        <div class="row" class='examples'>
            
            <div class="col s4 offset-s4"><img src='images/capture (5).png' /></div> 
        </div> -->
        <div class="row"></div>
            <div class="col m11 offset-m1 headTitle" style="font-size:180%">
            Coding of the dataset
            <hr>
            </div>
        </div>

        
    </div>

    
    
    <div id="resultsGraph"></div>
    <div class="row">

        <div class="col m6 offset-m3">
            <img src="visualization/Viz/legend.png" style="box-shadow:none">
        </div>
    </div>

</body>
<script src="visualization/index.js"></script>
    
</html>
